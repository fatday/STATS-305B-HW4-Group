{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatday/STATS-305B-HW4-Group/blob/main/Part2_Working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 Code Set up"
      ],
      "metadata": {
        "id": "2mw3nm5WWLGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_5IXGh6OOBZV"
      },
      "outputs": [],
      "source": [
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "\n",
        "torch.manual_seed(305)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgHl9JCuGBS"
      },
      "source": [
        "We set default values for some global hyperparameters, but feel free to change these during development as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A_Z5Jh74DH_E"
      },
      "outputs": [],
      "source": [
        "# Global hyperparameters\n",
        "SMALL_ITERS = 1000\n",
        "LARGE_ITERS = 2000\n",
        "EVAL_ITERS = 100\n",
        "CONTEXT_WINDOW_SIZE = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6dgHnhOprg"
      },
      "source": [
        "## Part 0: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0_bhXNOxeS"
      },
      "source": [
        "### 0.1: Loading and preprocessing the dataset\n",
        "\n",
        "\n",
        "The first step is to download the dataset. We will be using a dataset from Andrej Karpathy consisting of a subset of works from Shakespeare.\n",
        "\n",
        "The dominant mode for preprocessing textual data is to tokenize it; that is, to split the dataset into a finite vocabulary of tokens. Then, we can set up a dictionaries mapping from counting numbers (representing tokens) to tokens and vice versa. Tokens can be characters, or words, or subwords; in fact, the \"best\" way to tokenize text is an active area of research.\n",
        "\n",
        "To keep things simple, we'll tokenize the text on a per-character level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "53dGz7ExDkUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2edbe03-f1b9-43dc-cbe2-6da5aefcc309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n"
          ]
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset\n",
        "input_file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Classes"
      ],
      "metadata": {
        "id": "1EFRLq9wq2dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "        self.vocab_size = 0\n",
        "        self.train_data = None\n",
        "        self.val_data = None\n",
        "\n",
        "    def encode(self, s):\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l):\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "\n",
        "    def get_batch(self, split, context_window_size, device, batch_size=32):\n",
        "\n",
        "        current_data = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(data) - context_window_size, (batch_size,))\n",
        "        x = torch.stack([current_data[i:i+context_window_size] for i in ix])\n",
        "        y = torch.stack([current_data[i+1:i+context_window_size+1] for i in ix])\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        return x, y\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def estimate_loss(self, model, eval_iters, context_window_size, device):\n",
        "        # estimate loss, perplexity, character_level perplexity\n",
        "        lossout, perplexity, char_perplexity = {}, {}, {}\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            perplexities = torch.zeros(eval_iters)\n",
        "            nll = 0\n",
        "            char_cnt = 0\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = self.get_batch(split, context_window_size, device)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "                perplexities[k] = torch.exp(loss).item()\n",
        "                nll += loss.item()\n",
        "                char_cnt += len(self.decode(Y[0].tolist()))\n",
        "\n",
        "            char_perplexity[split] = torch.exp(torch.tensor(nll / char_cnt)).item()\n",
        "            lossout[split] = losses.mean()\n",
        "            perplexity[split] = perplexities.mean()\n",
        "        return lossout\n"
      ],
      "metadata": {
        "id": "LhUpAqzsty-L"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub-Classes"
      ],
      "metadata": {
        "id": "BYWqxLNMuhqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTokenizer(Tokenizer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.chars = sorted(list(set(data)))\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        self.train_chars = data[:int(len(data)*0.9)]\n",
        "        self.val_chars = data[int(len(data)*0.9):]\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
        "\n",
        "        self.train_data = torch.tensor(self.encode(self.train_chars))\n",
        "        self.val_data = torch.tensor(self.encode(self.val_chars))"
      ],
      "metadata": {
        "id": "XIZm79ULq401"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleWordTokenizer(Tokenizer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.data_words = re.findall(r'\\w+|\\s+|[^\\w\\s]', data)\n",
        "        self.vocab_set = set(self.data_words).union(set(data))\n",
        "        self.vocab_size = len(self.vocab_set)\n",
        "\n",
        "        self.stoi = {ch:i for i,ch in enumerate(self.vocab_set)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(self.vocab_set)}\n",
        "\n",
        "        self.train_words = self.data_words[:int(len(self.data_words)*0.9)]\n",
        "        self.val_words = self.data_words[int(len(self.data_words)*0.9):]\n",
        "\n",
        "        self.train_data = torch.tensor(self.encode(self.train_words))\n",
        "        self.val_data = torch.tensor(self.encode(self.val_words))"
      ],
      "metadata": {
        "id": "pPM6bzJlto1Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer(Tokenizer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        # BPE\n",
        "        spm.SentencePieceTrainer.Train(input=\"input.txt\",\n",
        "        model_prefix=\"tokenizer\",\n",
        "        vocab_size=3000,\n",
        "        model_type=\"bpe\",\n",
        "        normalization_rule_name=\"identity\",\n",
        "        character_coverage=1.0,\n",
        "        add_dummy_prefix=False,\n",
        "        user_defined_symbols = ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?'])\n",
        "\n",
        "        self.sp = spm.SentencePieceProcessor(model_file=\"tokenizer.model\")\n",
        "        self.vocab_size = self.sp.vocab_size()\n",
        "\n",
        "\n",
        "        self.train_chars = data[:int(len(data)*0.9)]\n",
        "        self.val_chars = data[int(len(data)*0.9):]\n",
        "        self.train_data = torch.tensor(self.encode(self.train_chars))\n",
        "        self.val_data = torch.tensor(self.encode(self.val_chars))\n",
        "\n",
        "    def encode(self, s):\n",
        "      return self.sp.encode(s, out_type=int)\n",
        "\n",
        "    def decode(self, l):\n",
        "      return self.sp.decode(l)"
      ],
      "metadata": {
        "id": "e_iYFJd0q2Ap"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_eBPiT-Yy0q"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "        # TODO: your code here\n",
        "        B,T,D = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * self.head_size**-0.5\n",
        "        #tril = torch.tril(torch.ones(T, T, device=x.device))\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)  ## wei.shape:\n",
        "        out = wei @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REr3aWnS1xJL"
      },
      "outputs": [],
      "source": [
        "class SingleHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, head_size, embed_size=384):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        vocab_size: int, size of the vocabulary (V)\n",
        "        context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "        head_size: int, size of the head embedding dimension (K)\n",
        "        embed_size: int, size of the token embedding dimension (D)\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "      # TODO: your code below\n",
        "      self.atten_head = Head(head_size, context_window_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry\n",
        "                     in the batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V) logits[b,t] gives the length V vector of logits for the next token\n",
        "                   prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
        "        x = tok_emb + pos_emb # (B,T,D)\n",
        "        x = self.atten_head(x) # (B,T,D)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "\n",
        "        # TODO: your code here\n",
        "        B, T, V = logits.shape\n",
        "        logits = logits.view(B*T, V)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            targets = targets.view(B*T)\n",
        "            loss = -torch.mean(torch.log(F.softmax(logits, dim=1)[torch.arange(B*T), targets]))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "        # your code below\n",
        "        B, T = token_ids.shape\n",
        "        new_token_sequences = torch.zeros((B, T+max_new_tokens), dtype=torch.long, device=token_ids.device)\n",
        "        new_token_sequences[:, :T] = token_ids\n",
        "        for t in range(max_new_tokens):\n",
        "          input_tokens = new_token_sequences[:, max(0, T + t - self.context_window_size): T + t]\n",
        "          logits, loss = self(input_tokens)\n",
        "          logits = logits.view(B, min(T + t, self.context_window_size), -1)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          new_token = torch.multinomial(probs, num_samples=1)\n",
        "          new_token_sequences[:, T + t] = new_token.squeeze(-1)\n",
        "        return new_token_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vb8NU_s6Vfg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, context_window_size, num_heads, embed_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = embed_size // num_heads  # 确保总维度匹配\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [Head(self.head_size, context_window_size, embed_size) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.proj = nn.Linear(embed_size * num_heads, embed_size)  # 确保总维度匹配\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # 拼接 heads\n",
        "\n",
        "        out = self.dropout(self.proj(out))  # 投影回 embed_size\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvWHwcCzI1yr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "      super().__init__()\n",
        "      self.head_size = embed_size // num_heads\n",
        "      self.context_window_size = context_window_size\n",
        "      # TODO: your code below\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
        "                     batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
        "                  prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        B, T = token_ids.shape\n",
        "        tok_emb = self.token_embedding_table(token_ids)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.atten_heads(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B*T, -1)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = -torch.mean(torch.log(F.softmax(logits, dim=1)[torch.arange(B*T), targets]))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        B, T = token_ids.shape\n",
        "        new_token_sequences = torch.zeros((B, T+max_new_tokens), dtype=torch.long, device=token_ids.device)\n",
        "        new_token_sequences[:, :T] = token_ids\n",
        "        for t in range(max_new_tokens):\n",
        "          input_tokens = new_token_sequences[:, max(0, T + t - self.context_window_size): T + t]\n",
        "          logits, loss = self(input_tokens)\n",
        "          logits = logits.view(B,min(T + t, self.context_window_size), -1)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          new_token = torch.multinomial(probs, num_samples=1)\n",
        "          new_token_sequences[:, T + t] = new_token.squeeze(-1)\n",
        "        return new_token_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GbGqwKWJzOK"
      },
      "outputs": [],
      "source": [
        "# run this cell to initialize this deep learning module that you should use in the code your write later\n",
        "# you don't need to edit this layer\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\n",
        "        Given to you, you don't need to write any code here!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUDbIv9eISkf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        self.feed_forward = FeedForward(embed_size)\n",
        "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length\n",
        "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2veTg9N3ufJ"
      },
      "outputs": [],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        self.context_window_size = context_window_size\n",
        "\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "        x = tok_emb + pos_emb # (B, T, D)\n",
        "\n",
        "        # TODO: your code below\n",
        "        logits = ...\n",
        "        loss = ...\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        B, T, V = logits.shape\n",
        "        logits = logits.view(B*T, V)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "        \"\"\"\n",
        "        # TOOD, your code below\n",
        "        B, T = token_ids.shape\n",
        "        context_length = self.position_embedding_table.num_embeddings\n",
        "        new_token_sequences = torch.zeros((B, T+max_new_tokens), dtype=torch.long, device=token_ids.device)\n",
        "        new_token_sequences[:, :T] = token_ids\n",
        "        for t in range(max_new_tokens):\n",
        "            input_tokens = new_token_sequences[:, max(0,T + t - context_length):T + t]\n",
        "            logits, loss = self(input_tokens)\n",
        "            logits = logits.view(B,min(T + t, self.context_window_size), -1)\n",
        "            logits = logits[:, -1, :] # (B, V)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, V)\n",
        "            new_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            new_token_sequences[:, T + t] = new_token.squeeze(-1) # (B, T+1)\n",
        "        return new_token_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Tokenizer System -- (model, tokenizer) pair"
      ],
      "metadata": {
        "id": "8OgNoxvFwpg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For model training and text generation"
      ],
      "metadata": {
        "id": "n4TxF9Pow1A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTokenizerSystem:\n",
        "  def __init__(self, model, tokenizer, device, optimizer_lr = 1e-4,\n",
        "               optimizer_weight_decay = 1e-1):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.device = device\n",
        "    self.optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                       lr=optimizer_lr,\n",
        "                                       weight_decay=optimizer_weight_decay)\n",
        "\n",
        "\n",
        "  def train(self, lr = 1e-4, eval_interval = 20, display_interval = 200,\n",
        "            max_iter = 3000, early_stopping = False, min_delta = 0.001, verbose = False ):\n",
        "\n",
        "    best_score = None\n",
        "    self.loss_list = []\n",
        "    self.losses_train_val = {\"train\": [], \"val\": []}\n",
        "    self.perplexities_train_val = {\"train\": [], \"val\": []}\n",
        "    self.char_perplexities_train_val = {\"train\": [], \"val\": []}\n",
        "    for it in tqdm(range(max_iter)):\n",
        "      if verbose and (it % eval_interval == 0 or it == max_iter - 1):\n",
        "        print(f\"iteration {it}\")\n",
        "        losses, perplexity, char_perplexity = self.tokenizer.estimate_loss(self.model, EVAL_ITERS, self.model.context_window_size, self.device)\n",
        "\n",
        "        if best_score is None:\n",
        "          best_score = losses[\"val\"]\n",
        "\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        self.losses_train_val[\"train\"].append(losses[\"train\"])\n",
        "        self.losses_train_val[\"val\"].append(losses[\"val\"])\n",
        "        self.perplexities_train_val[\"train\"].append(perplexity[\"train\"])\n",
        "        self.perplexities_train_val[\"val\"].append(perplexity[\"val\"])\n",
        "        self.char_perplexities_train_val[\"train\"].append(char_perplexity[\"train\"])\n",
        "        self.char_perplexities_train_val[\"val\"].append(char_perplexity[\"val\"])\n",
        "        if abs(losses[\"val\"] - best_score) < min_delta:\n",
        "          print(f\"Early Stopping Triggered at iteration {it}\")\n",
        "          break\n",
        "        else:\n",
        "          best_score = losses[\"val\"]\n",
        "\n",
        "      if verbose and (it % display_interval == 0 or it == max_iter - 1):\n",
        "        print(f\"iteration {it}\")\n",
        "        train_loss, val_loss = self.losses_train_val[\"train\"][-1], self.losses_train_val[\"val\"][-1]\n",
        "        print(f\"step {it}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
        "\n",
        "      xb, yb = self.get_batch('train', self.model.context_window_size, self.device)\n",
        "      logits, loss = self.model(xb, yb)\n",
        "      self.loss_list.append(loss.detach().item())\n",
        "      self.optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  def generate(self, context: str, max_new_tokens = 256) -> str:\n",
        "    # given string, return out the prediction (input string included)\n",
        "    context_tokens = torch.tensor(self.tokenizer.encode(context), device=self.device).reshape(1, -1)\n",
        "    output_tokens = self.model.generate(context_tokens, max_new_tokens)[0].tolist()\n",
        "    output_context = self.tokenizer.decode(output_tokens)\n",
        "    return output_context\n"
      ],
      "metadata": {
        "id": "kZ62_46twBk0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Izr1wTOjzlo"
      },
      "source": [
        "## Part 2: Mini-Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF3jFrQj1f4"
      },
      "source": [
        "Quick recap: So far we have\n",
        "\n",
        "1. Preprocessed the Shakespeare dataset by encoding individual characters into integer tokens.\n",
        "2. Implemented single headed attention and then further generalized to multiheaded attention. We further combined multiheaded attention with deep learning to create the transformer architecture.\n",
        "3. Trained our transformer and generated output that looks to be in the style of Shakespeare.\n",
        "\n",
        "Up to this point, the performance of our simple language model has clearly made a lot of progress. We can see that our model has learned to generate text that is close to the style of Shakespeare, although there are still many quirks and room for improvement.\n",
        "\n",
        "### Project Outline\n",
        "\n",
        "Find some area of possible improvement.\n",
        "We interpret \"improvement\" quite loosely, but please state precisely why your proposed innovation might improve the model, and provide evidence that it does (or does not!) improve.\n",
        "For your idea, **formulate a hypothesis** for why this change should result in a better model. **Implement your changes** and **report any findings**.\n",
        "\n",
        "_Notes_: As this assignment is being treated as a project, you should expect training to take longer than previous assignments. However, please use your judgement to decide what is reasonable. We will not expect you to run training procedures that take more than 2 hours on the free Google Colab computing resources and we certainly do not expect you to acquire additional compute. The proposed improvements should not solely rely on increased computing demands.\n",
        "\n",
        "_Hints_: There are many aspects to assessing a model. For example, not only is quality of generated text important, it is also of interest to reduce costs associated with training.\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "In addition to a pdf of your python notebook, the submission for this project will be a written report no more than 4 pages in length using the [NeurIPS LaTex template](https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles). Your report should include detailed analysis of the hypotheses you chose to test along with any conclusions.\n",
        "\n",
        "The page limit for the report does not include bibliography or appendices. Make sure to keep the \"ready for submission\" option to help us grade anonymously. Your writeup should also contain a link to any code used to generate the project so that we can reference it while grading (Google Drive folder with colab notebooks or Github repo are both fine). You should have at least one plot in your main text (which is capped at 4 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7f7wY9I9jSF"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "You will generate two PDFs: one from Part 1, which involves completing this Colab to create a transformer baseline; and one from the mini-project in Part 2, which will be your write-up of no longer than 4 pages. Be sure to include a link to your code for Part 2 somewhere in your writeup.\n",
        "\n",
        "**Combine the two PDFs into a single PDF and submit on gradescope. Tag your PDF correctly.**\n",
        "\n",
        "If you work in a group of two, submit one assignment on gradescope and tag your group members. If you complete the assignment individually, submit as usual."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}