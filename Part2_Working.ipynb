{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatday/STATS-305B-HW4-Group/blob/main/Part2_Working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 Code Set up"
      ],
      "metadata": {
        "id": "2mw3nm5WWLGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_5IXGh6OOBZV"
      },
      "outputs": [],
      "source": [
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "\n",
        "torch.manual_seed(305)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgHl9JCuGBS"
      },
      "source": [
        "We set default values for some global hyperparameters, but feel free to change these during development as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A_Z5Jh74DH_E"
      },
      "outputs": [],
      "source": [
        "# Global hyperparameters\n",
        "SMALL_ITERS = 1000\n",
        "LARGE_ITERS = 2000\n",
        "EVAL_ITERS = 100\n",
        "CONTEXT_WINDOW_SIZE = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6dgHnhOprg"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "53dGz7ExDkUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3be12e-299b-4480-e119-3306beaf5734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n"
          ]
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset\n",
        "input_file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and its Sub-Classes"
      ],
      "metadata": {
        "id": "1EFRLq9wq2dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "        self.vocab_size = 0\n",
        "        self.train_data = None\n",
        "        self.val_data = None\n",
        "\n",
        "    def encode(self, s):\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l):\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "\n",
        "    def get_batch(self, split, context_window_size, device, batch_size=32):\n",
        "\n",
        "        current_data = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(current_data) - context_window_size, (batch_size,))\n",
        "        x = torch.stack([current_data[i:i+context_window_size] for i in ix])\n",
        "        y = torch.stack([current_data[i+1:i+context_window_size+1] for i in ix])\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        return x, y\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def estimate_loss(self, model, eval_iters, context_window_size, device):\n",
        "        # estimate loss, perplexity, character_level perplexity\n",
        "        lossout, perplexity, char_perplexity = {}, {}, {}\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            perplexities = torch.zeros(eval_iters)\n",
        "            nll = 0\n",
        "            char_cnt = 0\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = self.get_batch(split, context_window_size, device)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "                perplexities[k] = torch.exp(loss).item()\n",
        "                nll += loss.item()\n",
        "                char_cnt += len(self.decode(Y[0].tolist()))\n",
        "\n",
        "            char_perplexity[split] = torch.exp(torch.tensor(nll / char_cnt)).item()\n",
        "            lossout[split] = losses.mean()\n",
        "            perplexity[split] = perplexities.mean()\n",
        "        return lossout, perplexity, char_perplexity\n"
      ],
      "metadata": {
        "id": "LhUpAqzsty-L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTokenizer(Tokenizer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.chars = sorted(list(set(data)))\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        self.train_chars = data[:int(len(data)*0.9)]\n",
        "        self.val_chars = data[int(len(data)*0.9):]\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
        "\n",
        "        self.train_data = torch.tensor(self.encode(self.train_chars))\n",
        "        self.val_data = torch.tensor(self.encode(self.val_chars))"
      ],
      "metadata": {
        "id": "XIZm79ULq401"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleWordTokenizer(Tokenizer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.data_words = re.findall(r'\\w+|\\s+|[^\\w\\s]', data)\n",
        "        self.vocab_set = set(self.data_words).union(set(data))\n",
        "        self.vocab_size = len(self.vocab_set)\n",
        "\n",
        "        self.stoi = {ch:i for i,ch in enumerate(self.vocab_set)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(self.vocab_set)}\n",
        "\n",
        "        self.train_words = self.data_words[:int(len(self.data_words)*0.9)]\n",
        "        self.val_words = self.data_words[int(len(self.data_words)*0.9):]\n",
        "\n",
        "        self.train_data = torch.tensor(self.encode(self.train_words))\n",
        "        self.val_data = torch.tensor(self.encode(self.val_words))"
      ],
      "metadata": {
        "id": "pPM6bzJlto1Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer(Tokenizer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        # BPE\n",
        "        spm.SentencePieceTrainer.Train(input=\"input.txt\",\n",
        "        model_prefix=\"tokenizer\",\n",
        "        vocab_size=3000,\n",
        "        model_type=\"bpe\",\n",
        "        normalization_rule_name=\"identity\",\n",
        "        character_coverage=1.0,\n",
        "        add_dummy_prefix=False,\n",
        "        user_defined_symbols = ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?'])\n",
        "\n",
        "        self.sp = spm.SentencePieceProcessor(model_file=\"tokenizer.model\")\n",
        "        self.vocab_size = self.sp.vocab_size()\n",
        "\n",
        "\n",
        "        self.train_chars = data[:int(len(data)*0.9)]\n",
        "        self.val_chars = data[int(len(data)*0.9):]\n",
        "        self.train_data = torch.tensor(self.encode(self.train_chars))\n",
        "        self.val_data = torch.tensor(self.encode(self.val_chars))\n",
        "\n",
        "    def encode(self, s):\n",
        "      return self.sp.encode(s, out_type=int)\n",
        "\n",
        "    def decode(self, l):\n",
        "      return self.sp.decode(l)"
      ],
      "metadata": {
        "id": "e_iYFJd0q2Ap"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Head"
      ],
      "metadata": {
        "id": "Os16M0Gii4N-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O_eBPiT-Yy0q"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "        # TODO: your code here\n",
        "        B,T,D = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * self.head_size**-0.5\n",
        "        #tril = torch.tril(torch.ones(T, T, device=x.device))\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)  ## wei.shape:\n",
        "        out = wei @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "REr3aWnS1xJL"
      },
      "outputs": [],
      "source": [
        "class SingleHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, head_size, embed_size=384):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        vocab_size: int, size of the vocabulary (V)\n",
        "        context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "        head_size: int, size of the head embedding dimension (K)\n",
        "        embed_size: int, size of the token embedding dimension (D)\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "      # TODO: your code below\n",
        "      self.atten_head = Head(head_size, context_window_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry\n",
        "                     in the batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V) logits[b,t] gives the length V vector of logits for the next token\n",
        "                   prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
        "        x = tok_emb + pos_emb # (B,T,D)\n",
        "        x = self.atten_head(x) # (B,T,D)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "\n",
        "        # TODO: your code here\n",
        "        B, T, V = logits.shape\n",
        "        logits = logits.view(B*T, V)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            targets = targets.view(B*T)\n",
        "            loss = -torch.mean(torch.log(F.softmax(logits, dim=1)[torch.arange(B*T), targets]))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "        # your code below\n",
        "        B, T = token_ids.shape\n",
        "        new_token_sequences = torch.zeros((B, T+max_new_tokens), dtype=torch.long, device=token_ids.device)\n",
        "        new_token_sequences[:, :T] = token_ids\n",
        "        for t in range(max_new_tokens):\n",
        "          input_tokens = new_token_sequences[:, max(0, T + t - self.context_window_size): T + t]\n",
        "          logits, loss = self(input_tokens)\n",
        "          logits = logits.view(B, min(T + t, self.context_window_size), -1)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          new_token = torch.multinomial(probs, num_samples=1)\n",
        "          new_token_sequences[:, T + t] = new_token.squeeze(-1)\n",
        "        return new_token_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6vb8NU_s6Vfg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, context_window_size, num_heads, embed_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = embed_size // num_heads  # 确保总维度匹配\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [Head(self.head_size, context_window_size, embed_size) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.proj = nn.Linear(embed_size * num_heads, embed_size)  # 确保总维度匹配\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # 拼接 heads\n",
        "\n",
        "        out = self.dropout(self.proj(out))  # 投影回 embed_size\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LvWHwcCzI1yr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "      super().__init__()\n",
        "      self.head_size = embed_size // num_heads\n",
        "      self.context_window_size = context_window_size\n",
        "      # TODO: your code below\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size)\n",
        "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
        "                     batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
        "                  prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        B, T = token_ids.shape\n",
        "        tok_emb = self.token_embedding_table(token_ids)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.atten_heads(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B*T, -1)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = -torch.mean(torch.log(F.softmax(logits, dim=1)[torch.arange(B*T), targets]))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        B, T = token_ids.shape\n",
        "        new_token_sequences = torch.zeros((B, T+max_new_tokens), dtype=torch.long, device=token_ids.device)\n",
        "        new_token_sequences[:, :T] = token_ids\n",
        "        for t in range(max_new_tokens):\n",
        "          input_tokens = new_token_sequences[:, max(0, T + t - self.context_window_size): T + t]\n",
        "          logits, loss = self(input_tokens)\n",
        "          logits = logits.view(B,min(T + t, self.context_window_size), -1)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          new_token = torch.multinomial(probs, num_samples=1)\n",
        "          new_token_sequences[:, T + t] = new_token.squeeze(-1)\n",
        "        return new_token_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoE Head"
      ],
      "metadata": {
        "id": "NkRZivDeiRlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings=2048):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        self.max_seq_len_cached = max_position_embeddings\n",
        "        self._set_cos_sin_cache(max_position_embeddings)\n",
        "\n",
        "    def _set_cos_sin_cache(self, seq_len):\n",
        "        t = torch.arange(seq_len, device=self.inv_freq.device)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer('cos_cached', emb.cos()[None, None, :, :])\n",
        "        self.register_buffer('sin_cached', emb.sin()[None, None, :, :])\n",
        "        self.max_seq_len_cached = seq_len\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        # x: [batch, seq_len, dim]\n",
        "        if seq_len > self.max_seq_len_cached:\n",
        "            self._set_cos_sin_cache(seq_len)\n",
        "        return self.cos_cached[:, :, :seq_len, ...], self.sin_cached[:, :, :seq_len, ...]\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "# Modify the Head class to use RoPE\n",
        "class HeadWithRoPE(nn.Module):\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.rope = RotaryEmbedding(head_size)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Apply RoPE to queries and keys\n",
        "        cos, sin = self.rope(x, seq_len=T)\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # Rest of the attention mechanism remains the same\n",
        "        wei = q @ k.transpose(-2, -1) * self.head_size**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "AcAq_fG8iQOG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWithRoPE(nn.Module):\n",
        "    def __init__(self, context_window_size, num_heads, embed_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = embed_size // num_heads\n",
        "\n",
        "        # QKV projections\n",
        "        self.q_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.k_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        # RoPE\n",
        "        self.rope = RotaryEmbedding(self.head_size)\n",
        "        # Causal mask\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        # Split heads\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "        # Apply RoPE to queries and keys\n",
        "        cos, sin = self.rope(x, seq_len=T)\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # Attention\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        scores = scores.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Combine heads\n",
        "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, D)\n",
        "        out = self.out_proj(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yHoqKLwtiYcn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1GbGqwKWJzOK"
      },
      "outputs": [],
      "source": [
        "# run this cell to initialize this deep learning module that you should use in the code your write later\n",
        "# you don't need to edit this layer\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\n",
        "        Given to you, you don't need to write any code here!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, gelu = False, dropout = False):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.activation = nn.GELU() if gelu else nn.ReLU()\n",
        "        layers = [nn.Linear(embed_size, 4 * embed_size), self.activation]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        layers.append(nn.Linear(4 * embed_size, embed_size))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer and Blocks"
      ],
      "metadata": {
        "id": "gT4pXUn6jKr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hUDbIv9eISkf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, roe = False,\n",
        "                  gelu = False, dropout = False, rmsnorm = False):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.RMSNorm(embed_size) if rmsnorm else nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.RMSNorm(embed_size) if rmsnorm else nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        self.feed_forward = FeedForward(embed_size, gelu = gelu, dropout = dropout)\n",
        "        self.atten_heads = MultiHeadAttentionWithRoPE(context_window_size, num_heads, embed_size) if roe else MultiHeadAttention(context_window_size, num_heads, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length\n",
        "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t2veTg9N3ufJ"
      },
      "outputs": [],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6,\n",
        "                 roe = False, gelu = False, dropout = False, rmsnorm = False):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        if not roe:\n",
        "          self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads,\n",
        "                             roe = roe,\n",
        "                             gelu= gelu,\n",
        "                             dropout = dropout,\n",
        "                             rmsnorm = rmsnorm)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.RMSNorm(embed_size) if rmsnorm else nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        self.context_window_size = context_window_size\n",
        "        self.roe = roe\n",
        "        self.gelu = gelu\n",
        "        self.dropout = dropout\n",
        "        self.rmsnorm = rmsnorm\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        if not self.roe:\n",
        "          pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "          x = tok_emb + pos_emb # (B, T, D)\n",
        "        else:\n",
        "          x = tok_emb\n",
        "\n",
        "        # TODO: your code below\n",
        "        logits = ...\n",
        "        loss = ...\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        B, T, V = logits.shape\n",
        "        logits = logits.view(B*T, V)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "        \"\"\"\n",
        "        # TOOD, your code below\n",
        "        B, T = token_ids.shape\n",
        "        context_length = self.position_embedding_table.num_embeddings\n",
        "        new_token_sequences = torch.zeros((B, T+max_new_tokens), dtype=torch.long, device=token_ids.device)\n",
        "        new_token_sequences[:, :T] = token_ids\n",
        "        for t in range(max_new_tokens):\n",
        "            input_tokens = new_token_sequences[:, max(0,T + t - context_length):T + t]\n",
        "            logits, loss = self(input_tokens)\n",
        "            logits = logits.view(B,min(T + t, self.context_window_size), -1)\n",
        "            logits = logits[:, -1, :] # (B, V)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, V)\n",
        "            new_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            new_token_sequences[:, T + t] = new_token.squeeze(-1) # (B, T+1)\n",
        "        return new_token_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model & Tokenizer System -- (model, tokenizer) pair"
      ],
      "metadata": {
        "id": "8OgNoxvFwpg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For model training and text generation"
      ],
      "metadata": {
        "id": "n4TxF9Pow1A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTokenizerSystem:\n",
        "  def __init__(self, model, tokenizer, device, optimizer_lr = 1e-4,\n",
        "               optimizer_weight_decay = 1e-1, model_name = \"None\"):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.device = device\n",
        "    self.optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                       lr=optimizer_lr,\n",
        "                                       weight_decay=optimizer_weight_decay)\n",
        "    self.model_name = model_name\n",
        "    self.weight_decay = optimizer_weight_decay\n",
        "\n",
        "  def train(self, lr = 1e-3, eval_interval = 50, display_interval = 200,\n",
        "            max_iter = 2000, early_stopping = True, min_delta = 0.001, verbose = True ):\n",
        "\n",
        "    best_score = None\n",
        "    self.loss_list = []\n",
        "    self.losses_train_val = {\"train\": [], \"val\": []}\n",
        "    self.perplexities_train_val = {\"train\": [], \"val\": []}\n",
        "    self.char_perplexities_train_val = {\"train\": [], \"val\": []}\n",
        "    for it in tqdm(range(max_iter)):\n",
        "      if verbose and (it % eval_interval == 0 or it == max_iter - 1):\n",
        "        losses, perplexity, char_perplexity = self.tokenizer.estimate_loss(self.model, EVAL_ITERS, self.model.context_window_size, self.device)\n",
        "        if best_score is None:\n",
        "          best_score = losses[\"val\"]\n",
        "        self.losses_train_val[\"train\"].append(losses[\"train\"].item())\n",
        "        self.losses_train_val[\"val\"].append(losses[\"val\"].item())\n",
        "        self.perplexities_train_val[\"train\"].append(perplexity[\"train\"].item())\n",
        "        self.perplexities_train_val[\"val\"].append(perplexity[\"val\"].item())\n",
        "        self.char_perplexities_train_val[\"train\"].append(char_perplexity[\"train\"])\n",
        "        self.char_perplexities_train_val[\"val\"].append(char_perplexity[\"val\"])\n",
        "\n",
        "        if early_stopping and abs(losses[\"val\"] - best_score) < min_delta:\n",
        "          print(f\"Early Stopping Triggered at iteration {it}\")\n",
        "          break\n",
        "        else:\n",
        "          best_score = losses[\"val\"]\n",
        "\n",
        "      if verbose and (it % display_interval == 0 or it == max_iter - 1):\n",
        "        print(f\"iteration {it}\")\n",
        "        train_loss, val_loss = self.losses_train_val[\"train\"][-1], self.losses_train_val[\"val\"][-1]\n",
        "        print(f\"step {it}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
        "\n",
        "      xb, yb = self.tokenizer.get_batch('train', self.model.context_window_size, self.device)\n",
        "      logits, loss = self.model(xb, yb)\n",
        "      self.loss_list.append(loss.detach().item())\n",
        "      self.optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "\n",
        "  def save_model_data(self):\n",
        "    folder_name = self.model_name\n",
        "\n",
        "    if self.model.roe:\n",
        "      folder_name += \"_RoE\"\n",
        "\n",
        "    if self.model.rmsnorm:\n",
        "      folder_name += \"_RMSNorm\"\n",
        "\n",
        "    if self.model.gelu:\n",
        "      folder_name += \"_GELU\"\n",
        "\n",
        "    if self.model.dropout:\n",
        "      folder_name += \"_Dropout\"\n",
        "\n",
        "    if self.weight_decay != 0:\n",
        "      folder_name += f\"_wd{self.weight_decay}\"\n",
        "\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "    model_path = os.path.join(folder_name, \"model.pth\")\n",
        "    torch.save(self.model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    data = {\n",
        "        \"loss_list\": self.loss_list,\n",
        "        \"losses_train_val\": self.losses_train_val,\n",
        "        \"perplexities_train_val\": self.perplexities_train_val,\n",
        "        \"char_perplexities_train_val\": self.char_perplexities_train_val\n",
        "    }\n",
        "    data_path = os.path.join(folder_name, \"training_data.json\")\n",
        "    with open(data_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"Training data saved to {data_path}\")\n",
        "\n",
        "\n",
        "  def generate(self, context: str, max_new_tokens = 256) -> str:\n",
        "    # given string, return out the prediction (input string included)\n",
        "    context_tokens = torch.tensor(self.tokenizer.encode(context), device=self.device).reshape(1, -1)\n",
        "    output_tokens = self.model.generate(context_tokens, max_new_tokens)[0].tolist()\n",
        "    output_context = self.tokenizer.decode(output_tokens)\n",
        "    return output_context\n"
      ],
      "metadata": {
        "id": "kZ62_46twBk0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Izr1wTOjzlo"
      },
      "source": [
        "## Part 2 Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Content"
      ],
      "metadata": {
        "id": "p_lHzHZ_1Y-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text1 = \"\"\"\n",
        "First Citizen:\n",
        "Care for us! True, indeed! They ne'er cared for us\n",
        "yet: suffer us to famish, and their store-houses\n",
        "crammed with grain; make edicts for usury, to\n",
        "support usurers; repeal daily any wholesome act\n",
        "established against the rich, and provide more\n",
        "piercing statutes daily, to chain up and restrain\n",
        "the poor. If the wars eat us not up, they will; and\n",
        "there's all the love they bear us.\n",
        "\"\"\"\n",
        "\n",
        "target_output1 = \"\"\"\n",
        "MENENIUS: Either you must\n",
        "Confess yourselves wondrous malicious,\n",
        "Or be accused of folly. I shall tell you\n",
        "A pretty tale: it may be you have heard it;\n",
        "But, since it serves my purpose, I will venture\n",
        "To stale 't a little more.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PJc6H_p-1ztC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text2 = \"\"\"MIRANDA:\n",
        "Why speaks my father so ungently? This\n",
        "Is the third man that e'er I saw, the first\n",
        "That e'er I sigh'd for: pity move my father\n",
        "To be inclined my way!\n",
        "\n",
        "FERDINAND:\n",
        "O, if a virgin,\n",
        "And your affection not gone forth, I'll make you\n",
        "The queen of Naples.\n",
        "\"\"\"\n",
        "\n",
        "target_output2 = \"\"\"PROSPERO:\n",
        "Soft, sir! one word more.\n",
        "They are both in either's powers; but this swift business\n",
        "I must uneasy make, lest too light winning\n",
        "Make the prize light.\n",
        "One word more; I charge thee\n",
        "That thou attend me: thou dost here usurp\n",
        "The name thou owest not; and hast put thyself\n",
        "Upon this island as a spy, to win it\n",
        "From me, the lord on't.\n",
        "\n",
        "FERDINAND:\n",
        "No, as I am a man.\n",
        "\n",
        "MIRANDA:\n",
        "There's nothing ill can dwell in such a temple:\n",
        "If the ill spirit have so fair a house,\n",
        "Good things will strive to dwell with't.\n",
        "\n",
        "PROSPERO:\n",
        "Follow me.\n",
        "Speak not you for him; he's a traitor. Come;\n",
        "I'll manacle thy neck and feet together:\n",
        "Sea-water shalt thou drink; thy food shall be\n",
        "The fresh-brook muscles, wither'd roots and husks\n",
        "Wherein the acorn cradled. Follow.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RG7T4mSf2RpG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model (model in Part 1)"
      ],
      "metadata": {
        "id": "HKPYkxcwq8Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "charTokenizer = CharacterTokenizer(data)\n",
        "baselineTransoformer = TransformerLM(charTokenizer.vocab_size, CONTEXT_WINDOW_SIZE).to(device)\n",
        "baseline = ModelTokenizerSystem(baselineTransoformer, charTokenizer, device, model_name= \"baseline\", optimizer_weight_decay=0)"
      ],
      "metadata": {
        "id": "-jQk5M9mqrBC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.train()"
      ],
      "metadata": {
        "id": "fmNFO54puTCG",
        "outputId": "cdecfa95-8d84-46f0-9d89-f9caeff33fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/2000 [00:06<3:34:32,  6.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "step 0: train loss nan, val loss nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▎         | 50/2000 [00:11<07:33,  4.30it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0f32c98dfc09>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-69637481b830>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, eval_interval, display_interval, max_iter, early_stopping, min_delta, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_ITERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0d0206709a48>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(self, model, eval_iters, context_window_size, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mperplexities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mnll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.save_model_data()"
      ],
      "metadata": {
        "id": "a3s2oZgL4DPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.generate(input_text1)"
      ],
      "metadata": {
        "id": "Ok3NRBvj4F58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.generate(input_text2)"
      ],
      "metadata": {
        "id": "hHyXzf-p9LU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppQwAnOp9DGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}